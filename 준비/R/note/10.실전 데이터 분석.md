# 실전 데이터 분석

## 실전 데이터 분석 예시
- 지역별 국내 휴양림 분포 (빈도분석)
- 해외 입국자 추이 확인
- 코로나 선별진료소 지도 시각화
- 서울시 지역별 미세먼지 농도


## 지역별 국내 휴양림 분포 
1. 데이터 수집 - 전국 휴양림 데이터 다운로드
   - https://www.data.go.kr
   - "전국휴양림표준데이터" 검색
   - [표준데이터셋] > "전국휴양림표준데이터"
   - [그리드] > [XLS]
  
2. 데이터 가공
   - 전처리
   * 데이터 항목 확인 : 휴양림명, 시도명, 휴양림구분, 휴양림면적, 수용인원수, 입장료, 숙박가능여부, 주요시설명 등
   * 비어있는 1행 삭제
   * 필요한 항목 : 휴양림명, 시도명, 휴양림구분, 휴양림면적, 수용인원수, 입장료, 숙박가능여부, 소재지도로명주소, 제공기관코드, 제공기관명
   * 불필요한 항목 : 입장료, 주요시설명, 관리기관명, 휴양림전화번호, 홈페이지주소, 위도, 경고, 데이터기준일
   * 새로운 항목 : "소재지도로명주소" 항목을 시와 도로 분리
      - [데이터] > [텍스트 나누기] > [구분 기호로 분리됨] > [탭, 공백] 
      - "기존 데이터를 바꾸시겠습니까?" > [확인]
      - "소재지도로명주소" → "소재지_시도명" 으로 항목명 변경
      - 시도명 아래 항목 열들은 삭제
   * 데이터 유형 변경
      - "휴양림면적", "수용인원수" : "문자형" → "숫자" 변환
   * 엑셀파일 저장 : "forest_example_data.xls"

  
3. 데이터 분석
   - 빈도 분석
   - 시각화
   
``` # 엑셀 파일 가져오기
   install.packages("readxl")
   library(readxl)
   path <- "C:/KHM/R/code/source/"
   file <- paste0(path, "forest_example_data.xls")
   forest_data <- read_excel(file)

   # 변수명 변경
   colnames(forest_data) <- c("name", "city", "gubun", "area", "number", "stay", "city_new", "code", "codename")

   # 데이터 구조 및 앞부분 확인
   str(forest_data)
   head(forest_data)
```

``` # 시도별 휴양림 빈도분석 - freq() 함수
   library(descr)
   freq(forest_data$city, plot = T, main = 'city')
```


``` # 시도별 휴양림 빈도분석 - table() 함수
   city_table <- table(forest_data$city)
   city_table
   barplot(city_table)
```

```# 시도별 휴양림 수 구하고 내림차순 정렬하기
   library(dplyr)
   count(forest_data, city) %>% arrange(desc(n))
```
- count(forest_data, city) : 휴양림 데이터세트의 시도별 개수 → 데이터 프레임(city, n)
- %>% arrange( desc(n) )   : 앞에서 구한 데이터 프레임의 n(개수)을 기준으로 desc() 함수로 내림차순 정렬


``` # 시도별, 소재지_시도명, 제공기관명 별로 분포 확인하기
   library(dplyr)
   count(forest_data, city) %>% arrange(desc(n))
   count(forest_data, city_new) %>% arrange(desc(n))
   count(forest_data, codename) %>% arrange(desc(n))
```


## 해외 입국자 추이 확인하기
1. 데이터 수집
   - https://know.tour.go.kr
   - "2020년 입국자 수" 데이터 조회
   - [통계] > [관광객통계] > [입국관광통계]
   - 검색 조건
     - 통계유형 : 국적별 입국
     - 기간구분 : 2020년 01월 ~ 2020년 12월
   - [엑셀 내려받기] > [셀병합 유지]
2. 데이터 가공 
   1. 엑셀로 전처리
   - "국적별" 셀 [셀 병합 해제] > 2행 삭제
   - 삭제할 열 : "계" 
   - 삭제할 행 : 아시아주, 중동, 미주, 구주, 대양주, 아프리카주, 기타, 전체 (연두색 배경)
   - 엑셀파일 저장 : entrance_exam.xls

   2. 데이터 재구조화
``` # 엑셀 데이터 가져오기
   install.packages("readxl")
   library(readxl)
   path <- "C:/KHM/R/code/source/"
   file <- paste0(path, "entrance_exam.xls")
   entrance_data <- read_excel(file)

   # 데이터 구조 및 앞부분 확인
   str(entrance_data)
   head(entrance_data)

   # 변수명 영어로 변경
   colnames(entrance_data) <- c("country", "JAN", "FEB", "MAR", "APR", "MAY", "JUN", "JUL", "AUG", "SEP", "OCT", "NOV", "DEC")

   # 국가명 띄어쓰기 제거
   entrance_data$country <- gsub(" ", "", entrance_data$country)
   head(entrance_data)
```

``` 
   # 국가 개수 확인
   entrance_data |> nrow()

   # 1월기준 상위 5개국 추출하기
   # order(-entrance_data$JAN) : 1월 기준 내림차순
   # |> head(n = 5)  :1월 기준 내림차순한 데이터의 상위 5개
   top5_country <- entrance_data[order(-entrance_data$JAN),] |> head(n = 5)
   top5_country

```

```
   # 데이터 구조 재구조화하기
   # - 열로 나열된 월별 데이터(JAN, FEB, ..)를 행으로 변환
   install.packages("reshape2")
   library(reshape2)
   top5_melt <- melt(top5_country, id.vars='country', variable.name = 'mon')
   head(top5_melt)
```
3. 데이터 분석
- 2020년 월별 입국자 수 
  - 데이터 변화 추이             : 선 그래프
  - 상위 5개국 국가별 변화 추이  : 막대그래프, 누적 막대 그래프
  
``` # 2020년 월별 입국자 수 - 선 그래프
   install.packages("ggplot2")
   library(ggplot2)
   ggplot(top5_melt, aes(x = mon, y = value, group = country)) + 
      geom_line(aes(color = country)) +
      ggtitle("2020년 국적별 입국자 수 변화 추이") +
      scale_y_continuous(breaks = seq(0, 50000, 50000))
```

``` # 2020년 월별 입국자 수 - 막대 그래프
   install.packages("ggplot2")
   library(ggplot2)
   ggplot(top5_melt, aes(x = mon, y = value, fill = country)) + 
      geom_bar(stat = "identity", position = "dodge") +
      ggtitle("2020년 국적별 입국자 수 변화 추이") +
      scale_y_continuous(breaks = seq(0, 50000, 50000))
```

``` # 2020년 월별 입국자 수 - 누적 그래프
   install.packages("ggplot2")
   library(ggplot2)
   ggplot(top5_melt, aes(x = mon, y = value, fill = country)) + 
      geom_bar(stat = "identity", position = "stack") +
      ggtitle("2020년 국적별 입국자 수 변화 추이") +
      scale_y_continuous(breaks = seq(0, 50000, 50000))
```


## 코로나 선별 진료소 지도로 시각화하기
1. 데이터 수집
   - https://www.data.go.kr
   - "선별진료소 현황" 검색
   - [파일데이터] > "보건복지부_코로나19 선별진료소_현황"
   - [URL] > [링크] > [엑셀파일 다운로드]
   - 엑셀 파일명 : 선별진료소_현황.xls

2. 데이터 가공
   - 엑셀 파일 가져오기
   - 필요한 컬럼만 추출하기

   ```  
   # 엑셀 데이터 가져오기
   install.packages("readxl")
   library(readxl)
   path <- "C:/KHM/R/code/source/"
   file <- paste0(path, "선별진료소_현황.xls")
   clinic_data <- read_excel(file)

   # 필요한 컬럼만 추출하기
   data_raw <- clinic_data[,c(2:5)]     # 2~5열의 모든행 선택
   head(data_raw)

   # 데이터 추출 확인 및 컬럼명 변경
   names(data_raw)
   names(data_raw) <- c("state", "city", "name", "addr")
   names(data_raw)

   ```


3. 데이터 분석
   1. 빈도 분석
   2. 지도 시각화


   ```
      # 지역별(state) 선별 진료소 빈도 분석
      table(data_raw$state)

      # 지역별(state) 선별 진료소 빈도 - 막대 그래프
      barplot(table(data_raw$state))

      # 대전시 선별 진료소 빈도
      deajeon_data <- data_raw[data_raw$state == "대전",]
      head(deajeon_data)
      nrow(daejeon_data)
      
   ```


   ``` 
      # 대전시 선별 진료소 위도,경도 가져오기
      install.packages("ggmap")
      library(ggmap)
      ggmap_key <- "AIzaSyDa_9XcZixmykF2y95JyJCZravaIvqdPPU"
      register_google(ggmap_key)

      dajeon_data <- mutate_geocode(data = daejeon_data, location = addr, source = 'google')
      head(dajeon_data)
      head(dajeon_data$lon)
      head(dajeon_data$lat)

   ```

   ```
      # 대전시 선별 진료소 지도 시각화 - 산점도로 표시
      dajeon_map <- get_googlemap('대전', maptype = 'roadmap', zoom = 11)
      ggmap(dajeon_map) +
         geom_point(data = daejeon_data,
                    aes(x = lon, y = lat, color = factor(name)), size = 3)
   ```

   ```
      # 대전시 선별 진료소 지도 시각화 - 구글 마커로 표시
      daejeon_data_marker <- data.frame(daejeon_data$lon, daejeon_data$lat)
      dajeon_map <- get_googlemap('대전', maptype = 'roadmap', zoom = 11, markers = daejeon_data_marker)

      ggmap(dajeon_map) +
         geom_text(data = daejeon_data, aes(x = lon, y = lat),
                     size = 3, label = daejeon_data$name)
   ```


## 서울시 지역별 미세먼지 농도 차이 비교
1. 데이터수집
   - https://cleanair.go.kr (서울특별시 대기환경정보 페이지)
   - [대기질 통계]
   - 측정기간 : 2023년 5월 미세먼지 전체
   - [엑셀 다운로드]
  
2. 데이터 가공
   1. 엑셀로 전처리
   - 1~4행 삭제
   - "구분" 셀 병합 해제
   - B 열 삭제
   - 1행 날짜 서식 → 텍스트 서식 변경
   - 셀 데이터 선택 → 복사 → 아래에 붙여넣기(행/열바꿈 옵션)
   - 기존 데이터 삭제
   - "구분" → "날짜
   - 1일 → 2023-05-01 날짜형식으로 변경
   - 1일~31일 까지 자동채우기
   - 엑셀 파일 저장 : dustdata.xlsx

   2. 필요한 데이터 추출
   ```
      # 엑셀 데이터 가져오기
      install.packages("readxl")
      library(readxl)
      path <- "C:/KHM/R/code/source/"
      file <- paste0(path, "dustdata.xlsx")
      dust_data <- read_excel(file)


      # 성북구, 중구 데이터만 추출하기
      install.packages("dplyr")
      library(dplyr)
      dust_data_select <- dustdata[, c("날짜", "성북구", "중구")]
      View(dust_data_select)
      
      # 결측치 확인하기
      is.na( dust_data_select )
      sum(is.na( dust_data_select ))
      
   ```
3. 데이터 분석
   1. 데이터 탐색 및 시각화
   2. 가설 검정

   * 가설 검정 : "가설의 합당성을 판정하는 과정"
   - 통계적 추론을 사용하여 주장하고자 하는 가설이 데이터와 일치하는지를 검증하는 과정
   - 가설 검정 관련 함수
      * var.test(데이터1, 데이터2) 
         : 두 개의 독립적인 집단의 분산을 비교
         - p-value <=  0.05 : 의미가 있는 차이가 있다. ("분산이 다르다")
         - p-value >   0.05 : 의미가 있는 차이가 없다. ("분산이 같다")
      * t.test(데이터1, 데이터2, var.equal = 등분산성 가정 여부)
         : 두 개의 독립적인 집단의 평균을 비교
         - p-value <=  0.05 : 의미가 있는 차이가 있다. ("평균이 다르다")
         - p-value >   0.05 : 의미가 있는 차이가 없다. ("평균이 같다")

   - 가설 검정 과정
   1. 귀무가설, 대립가설 설정
      - 귀무 가설 : "주장이 효과가 없거나 차이가 없다"
      - 대립 가설 : "주장이 효과가 잇거나 차이가 있다"
   2. 유의수준 설정
      - 귀무 가설을 기각하기 위한 p-value 값의 기준을 설정
      - 일반적으로 0.05
   3. 검정 방법 선택
      - 평균의 차이를 비교하는 경우 : t검정 분석기법
  
   4. 통계량 계산
      - 평균의 차이를 비교하는 경우 : t검정 분석기법 - t.test() 함수 사용

   5. p-value 도출
      - p-value (probability value)
        : 계산된 통계량이 귀무가설에 부함하는 정도를 나타내는 값
         * 적절한 분석기법 함수를 사용하여 p-value 를 결과로 얻음

   6. 결론 도출
        * 0.05 보다 더 크면 클수록 통계량이 더욱 더 귀무가설에 부합함
        * 0.05 보다 작으면 통계적으로 의미있는 차이가 있다고 판단하고 귀무가설을 기각하고 대립가설을 채택함 

  

   ```
      # 지역별 미세먼지 농도 기술통계량 
      install.packages("psych")
      library(psych)

      describe( dust_data_select$성북구 )
      describe( dust_data_select$중구 )

      # 성북구와 중구 미세먼지 농도 상자 그림 그리기
      boxplot( dust_data_select$성북구, dust_data_select$중구,
               main = "finedust_compare", xlab = "AREA", namse = c("성북구", "중구"),
               ylab = "FINEDUST_PM", col = c("blue", "green"))                                                          

   ```

   ```
      # 지역별 미세번지 농도 분산 차이 검정하기
      var.test( dust_data_select$중구, dust_data_select$성북구 )

      # 지역별 평균 차이 검정하기
      t.test( dust_data_select$중구, dust_data_select$성북구, var.equal = T )

   ```


## 데이터 제공처
1. 공공데이터포털
   - https://www.data.go.kr/
2. LOCAL DATA
   - https://www.localdata.kr/
3. 서울특별시 대기환경정보
   - https://cleanair.seoul.go.kr/
4. 트위터 API
   - twitterR
   - https://developer.twitter.com/en/docs/platform-overview
5. 서울열린데이터광장
   - https://data.seoul.go.kr/
